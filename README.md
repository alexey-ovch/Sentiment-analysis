# Классификация текстовых отзывов методом ансамблирования

Проект посвящен разработке модели классификации текстовых отзывов с использованием ансамблирования. Задача заключается в определении принадлежности текстового отзыва к одному из двух классов на основе его содержимого.

## Датасет

### Исходные данные

Данные представлены в виде CSV-файлов:
- **train.csv** — обучающая выборка с текстами отзывов и метками класса
- **test.csv** — тестовая выборка для генерации предсказаний

### Структура данных

**Столбцы в train.csv:**
- `id` — уникальный идентификатор отзыва
- `Review` — текст отзыва
- `label` — целевая переменная (класс: 0 или 1)

**Столбцы в test.csv:**
- `id` — уникальный идентификатор отзыва
- `Review` — текст отзыва

## Предобработка текстовых данных

### 1. Обработка пропусков

Пропущенные значения в столбце `Review` заменяются пустыми строками. Это обеспечивает корректную работу векторизации без ошибок.

### 2. Векторизация текста (TF-IDF)

Применяется **TF-IDF** (Term Frequency-Inverse Document Frequency) с n-граммами: `vectorizer = TfidfVectorizer(ngram_range=(1,2))`

**Параметры:**
- `ngram_range=(1,2)` — использование однограмм (отдельных слов) и биграмм (пар слов)
- TF-IDF автоматически:
  - Вычисляет частоту терминов в каждом документе
  - Снижает вес часто встречаемых слов (stop-words)
  - Повышает вес редких и информативных слов

**Результат:** разреженная матрица, где каждая строка — документ, каждый столбец — терм, значение — TF-IDF вес.

### 3. L2-нормализация

Матрица TF-IDF нормализуется по евклидовой норме (L2)
**Зачем это нужно?**
- Приводит каждый вектор документа к единичной норме
- Делает модели более стабильными
- Улучшает производительность моделей на основе расстояний
- Важна для логистической регрессии

## Модели машинного обучения

### 1. Random Forest Classifier

Ансамбль деревьев решений: `rf_model = RandomForestClassifier(n_estimators=200, random_state=2)`

**Параметры:**
- `n_estimators=200` — количество деревьев в лесу
- `random_state=2` — seed для воспроизводимости

**Характеристики:**
- Хорошо работает с разреженными данными (TF-IDF матрица)
- Устойчив к переобучению благодаря усреднению множества деревьев
- Захватывает нелинейные зависимости между признаками

### 2. Logistic Regression

Линейная модель классификации: `lr_model = LogisticRegression(max_iter=200, random_state=2)`
**Параметры:**
- `max_iter=200` — максимальное количество итераций обучения
- `random_state=2` — seed для воспроизводимости

**Характеристики:**
- Быстрая и интерпретируемая модель
- Работает хорошо с линейно разделимыми данными
- Выдает хорошо откалиброванные вероятности

### 3. Voting Classifier (Ансамблирование)

Объединение двух моделей: `voting_model = VotingClassifier(estimators=[('rf', rf_model),('lr', lr_model)], voting='soft')`

**Параметры:**
- `estimators` — список моделей для ансамблирования
- `voting='soft'` — мягкое голосование на основе средних вероятностей

**Как это работает?**

С `voting='soft'`:
1. Каждая модель предсказывает вероятность принадлежности к классу 1
2. Вероятности усредняются: `P(class=1) = (P_rf + P_lr) / 2`
3. Выбирается класс с наибольшей усредненной вероятностью

**Почему ансамблирование?**
- Комбинирует сильные стороны обеих моделей
- Random Forest хорошо ловит сложные нелинейные паттерны
- Logistic Regression предоставляет линейную интерпретацию
- Обычно результат лучше, чем от отдельных моделей
- Снижает риск переобучения

## Метрики оценки

Обычные метрики для задачи бинарной классификации:

- **Accuracy** — доля правильно классифицированных примеров
- **Precision** — доля истинно положительных среди предсказанных положительных
- **Recall** — доля истинно положительных среди реально положительных
- **F1-score** — гармоническое среднее Precision и Recall
- **ROC-AUC** — площадь под ROC-кривой
- **Log Loss** — логарифмическая ошибка для вероятностей


## Процесс обучения и предсказания

### 1. Обучение ансамбля 
`voting_model.fit(X_train_tfidf, y_train)`

На этом этапе:
- Random Forest обучается на TF-IDF признаках
- Logistic Regression обучается на том же наборе признаков
- Модели обучаются независимо друг от друга

### 2. Получение вероятностей на тесте 
`y_pred_proba = voting_model.predict_proba(X_test_tfidf)`

`predict_proba` возвращает массив размерности (n_samples, n_classes):
- Первый столбец — вероятность класса 0
- Второй столбец — вероятность класса 1

### 3. Извлечение вероятностей для класса 1 
`test_data['probability'] = y_pred_proba[:, 1]`
Берутся вероятности принадлежности к классу 1 (второй столбец).

## Возможные улучшения
### Предобработка текста

- **Токенизация и лемматизация**: преобразование слов в начальную форму (например, "работают" → "работать")
- **Удаление stop-words**: исключение часто встречаемых слов, которые не несут смысла
- **Удаление пунктуации и символов**: очистка текста
- **Приведение к нижнему регистру**: стандартизация текста

### Оптимизация TF-IDF

- Экспериментирование с диапазонами n-грамм: `ngram_range=(1,3)` или `(2,3)`
- Изменение параметра `max_features` для ограничения количества признаков
- Использование других методов векторизации:
- **Word2Vec** — векторы слов, учитывающие семантику
- **GloVe** — глобальные векторы слов
- **BERT embeddings** — контекстные представления слов

### Добавление новых моделей

- **Gradient Boosting** (XGBoost, LightGBM, CatBoost) — часто показывают лучшие результаты
- **SVM** (Support Vector Machine) — мощная модель для классификации
- **Naive Bayes** — быстрая и хорошо работающая модель для текстов
- **Neural Networks** — LSTM, GRU, или трансформеры (BERT, RoBERTa)

### Кросс-валидация

- Добавление кросс-валидации на тренировочных данных для оценки качества:
`from sklearn.model_selection import cross_val_score`
`scores = cross_val_score(voting_model, X_train_tfidf, y_train, cv=5)`
`print(f"CV Score: {scores.mean():.4f} (+/- {scores.std():.4f})")`

### Гиперпараметрическая оптимизация
- GridSearchCV или RandomizedSearchCV для подбора оптимальных параметров:
- Количество деревьев в Random Forest
- Регуляризация в Logistic Regression (parameter `C`)
- Параметры TF-IDF (max_features, min_df, max_df)

### Анализ ошибок
- Изучение неправильно классифицированных примеров
- Feature importance анализ (какие слова наиболее важны)
- Анализ ROC-кривой и матрицы ошибок
